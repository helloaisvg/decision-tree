import numpy as np
from decision_tree_lab import *

def test_compute_entropy():
    """æµ‹è¯•ç†µè®¡ç®—å‡½æ•°"""
    print("æµ‹è¯•ç†µè®¡ç®—å‡½æ•°...")
    
    # æµ‹è¯•ç”¨ä¾‹1: æ‰€æœ‰æ ·æœ¬éƒ½æ˜¯å¯é£Ÿç”¨çš„
    y1 = np.array([1, 1, 1, 1, 1])
    entropy1 = compute_entropy(y1)
    expected1 = 0.0
    assert abs(entropy1 - expected1) < 1e-10, f"æœŸæœ›ç†µä¸º{expected1}, ä½†å¾—åˆ°{entropy1}"
    print("âœ“ æµ‹è¯•ç”¨ä¾‹1é€šè¿‡: æ‰€æœ‰æ ·æœ¬å¯é£Ÿç”¨")
    
    # æµ‹è¯•ç”¨ä¾‹2: æ‰€æœ‰æ ·æœ¬éƒ½æ˜¯æœ‰æ¯’çš„
    y2 = np.array([0, 0, 0, 0, 0])
    entropy2 = compute_entropy(y2)
    expected2 = 0.0
    assert abs(entropy2 - expected2) < 1e-10, f"æœŸæœ›ç†µä¸º{expected2}, ä½†å¾—åˆ°{entropy2}"
    print("âœ“ æµ‹è¯•ç”¨ä¾‹2é€šè¿‡: æ‰€æœ‰æ ·æœ¬æœ‰æ¯’")
    
    # æµ‹è¯•ç”¨ä¾‹3: ä¸€åŠæ ·æœ¬å¯é£Ÿç”¨ï¼Œä¸€åŠæœ‰æ¯’
    y3 = np.array([1, 1, 1, 0, 0, 0])
    entropy3 = compute_entropy(y3)
    expected3 = 1.0
    assert abs(entropy3 - expected3) < 1e-10, f"æœŸæœ›ç†µä¸º{expected3}, ä½†å¾—åˆ°{entropy3}"
    print("âœ“ æµ‹è¯•ç”¨ä¾‹3é€šè¿‡: ä¸€åŠæ ·æœ¬å¯é£Ÿç”¨")
    
    # æµ‹è¯•ç”¨ä¾‹4: ç©ºæ•°ç»„
    y4 = np.array([])
    entropy4 = compute_entropy(y4)
    expected4 = 0.0
    assert entropy4 == expected4, f"æœŸæœ›ç†µä¸º{expected4}, ä½†å¾—åˆ°{entropy4}"
    print("âœ“ æµ‹è¯•ç”¨ä¾‹4é€šè¿‡: ç©ºæ•°ç»„")
    
    print("æ‰€æœ‰ç†µè®¡ç®—æµ‹è¯•é€šè¿‡ï¼\n")

def test_split_dataset():
    """æµ‹è¯•æ•°æ®é›†åˆ†å‰²å‡½æ•°"""
    print("æµ‹è¯•æ•°æ®é›†åˆ†å‰²å‡½æ•°...")
    
    X = np.array([[1,1,1], [1,0,1], [0,1,0], [0,0,1]])
    node_indices = [0, 1, 2, 3]
    
    # æµ‹è¯•ç‰¹å¾0
    left_indices, right_indices = split_dataset(X, node_indices, 0)
    expected_left = [0, 1]
    expected_right = [2, 3]
    
    assert set(left_indices) == set(expected_left), f"æœŸæœ›å·¦ç´¢å¼•ä¸º{expected_left}, ä½†å¾—åˆ°{left_indices}"
    assert set(right_indices) == set(expected_right), f"æœŸæœ›å³ç´¢å¼•ä¸º{expected_right}, ä½†å¾—åˆ°{right_indices}"
    print("âœ“ ç‰¹å¾0åˆ†å‰²æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•ç‰¹å¾1
    left_indices, right_indices = split_dataset(X, node_indices, 1)
    expected_left = [0, 2]
    expected_right = [1, 3]
    
    assert set(left_indices) == set(expected_left), f"æœŸæœ›å·¦ç´¢å¼•ä¸º{expected_left}, ä½†å¾—åˆ°{left_indices}"
    assert set(right_indices) == set(expected_right), f"æœŸæœ›å³ç´¢å¼•ä¸º{expected_right}, ä½†å¾—åˆ°{right_indices}"
    print("âœ“ ç‰¹å¾1åˆ†å‰²æµ‹è¯•é€šè¿‡")
    
    print("æ‰€æœ‰æ•°æ®é›†åˆ†å‰²æµ‹è¯•é€šè¿‡ï¼\n")

def test_compute_information_gain():
    """æµ‹è¯•ä¿¡æ¯å¢ç›Šè®¡ç®—å‡½æ•°"""
    print("æµ‹è¯•ä¿¡æ¯å¢ç›Šè®¡ç®—å‡½æ•°...")
    
    # ä½¿ç”¨æ›´å¥½çš„æµ‹è¯•æ•°æ®ï¼Œç¡®ä¿æœ‰ä¿¡æ¯å¢ç›Š
    X = np.array([[1,1,1], [1,0,1], [0,1,0], [0,0,0]])
    y = np.array([1, 1, 0, 0])
    node_indices = [0, 1, 2, 3]
    
    # æµ‹è¯•ç‰¹å¾0çš„ä¿¡æ¯å¢ç›Š
    info_gain = compute_information_gain(X, y, node_indices, 0)
    assert info_gain > 0, "ä¿¡æ¯å¢ç›Šåº”è¯¥å¤§äº0"
    print("âœ“ ç‰¹å¾0ä¿¡æ¯å¢ç›Šæµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•ç‰¹å¾1çš„ä¿¡æ¯å¢ç›Š
    info_gain = compute_information_gain(X, y, node_indices, 1)
    assert info_gain > 0, "ä¿¡æ¯å¢ç›Šåº”è¯¥å¤§äº0"
    print("âœ“ ç‰¹å¾1ä¿¡æ¯å¢ç›Šæµ‹è¯•é€šè¿‡")
    
    print("æ‰€æœ‰ä¿¡æ¯å¢ç›Šæµ‹è¯•é€šè¿‡ï¼\n")

def test_get_best_split():
    """æµ‹è¯•æœ€ä½³åˆ†å‰²ç‰¹å¾é€‰æ‹©å‡½æ•°"""
    print("æµ‹è¯•æœ€ä½³åˆ†å‰²ç‰¹å¾é€‰æ‹©å‡½æ•°...")
    
    X = np.array([[1,1,1], [1,0,1], [0,1,0], [0,0,1]])
    y = np.array([1, 1, 0, 0])
    node_indices = [0, 1, 2, 3]
    
    best_feature = get_best_split(X, y, node_indices)
    assert best_feature in [0, 1, 2], f"æœ€ä½³ç‰¹å¾åº”è¯¥åœ¨[0,1,2]èŒƒå›´å†…ï¼Œä½†å¾—åˆ°{best_feature}"
    print(f"âœ“ æœ€ä½³åˆ†å‰²ç‰¹å¾æµ‹è¯•é€šè¿‡: ç‰¹å¾{best_feature}")
    
    print("æœ€ä½³åˆ†å‰²ç‰¹å¾æµ‹è¯•é€šè¿‡ï¼\n")

def test_mushroom_dataset():
    """æµ‹è¯•è˜‘è‡æ•°æ®é›†ä¸Šçš„å®Œæ•´æµç¨‹"""
    print("æµ‹è¯•è˜‘è‡æ•°æ®é›†ä¸Šçš„å®Œæ•´æµç¨‹...")
    
    # æµ‹è¯•æ ¹èŠ‚ç‚¹ç†µ
    root_entropy = compute_entropy(y_train)
    expected_entropy = 1.0  # 5ä¸ªå¯é£Ÿç”¨ï¼Œ5ä¸ªæœ‰æ¯’
    assert abs(root_entropy - expected_entropy) < 1e-10, f"æœŸæœ›æ ¹èŠ‚ç‚¹ç†µä¸º{expected_entropy}, ä½†å¾—åˆ°{root_entropy}"
    print("âœ“ æ ¹èŠ‚ç‚¹ç†µæµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•æœ€ä½³åˆ†å‰²ç‰¹å¾
    root_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
    best_feature = get_best_split(X_train, y_train, root_indices)
    expected_best = 2  # ç‹¬ç”Ÿç‰¹å¾åº”è¯¥æä¾›æœ€å¤§ä¿¡æ¯å¢ç›Š
    assert best_feature == expected_best, f"æœŸæœ›æœ€ä½³ç‰¹å¾ä¸º{expected_best}, ä½†å¾—åˆ°{best_feature}"
    print("âœ“ æœ€ä½³åˆ†å‰²ç‰¹å¾æµ‹è¯•é€šè¿‡")
    
    print("è˜‘è‡æ•°æ®é›†æµ‹è¯•é€šè¿‡ï¼\n")

def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("å¼€å§‹è¿è¡Œå†³ç­–æ ‘æµ‹è¯•...")
    print("=" * 50)
    
    try:
        test_compute_entropy()
        test_split_dataset()
        test_compute_information_gain()
        test_get_best_split()
        test_mushroom_dataset()
        
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼")
        print("å†³ç­–æ ‘å®ç°å®Œå…¨æ­£ç¡®ï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    run_all_tests()
