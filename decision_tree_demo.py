import numpy as np
import matplotlib.pyplot as plt
from decision_tree_lab import *
from utils import *

# è®¾ç½®matplotlibæ˜¾ç¤ºä¸­æ–‡
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def run_complete_demo():
    """è¿è¡Œå®Œæ•´çš„å†³ç­–æ ‘æ¼”ç¤º"""
    print("ğŸŒ³ å†³ç­–æ ‘å®è·µå®éªŒå®¤ - å®Œæ•´æ¼”ç¤º")
    print("=" * 60)
    
    # è®¾ç½®æ•°æ®é›†ä¾›å¯è§†åŒ–å‡½æ•°ä½¿ç”¨
    set_dataset(X_train, y_train)
    
    # 1. æ•°æ®é›†æ¦‚è§ˆ
    print("\nğŸ“Š æ•°æ®é›†æ¦‚è§ˆ")
    print("-" * 30)
    print_dataset_summary()
    
    # 2. è¿è¡Œæ‰€æœ‰ç»ƒä¹ 
    print("\nğŸ”¬ è¿è¡Œæ‰€æœ‰ç»ƒä¹ ")
    print("-" * 30)
    main()
    
    # 3. å¯è§†åŒ–ç†µå‡½æ•°
    print("\nğŸ“ˆ å¯è§†åŒ–ç†µå‡½æ•°")
    print("-" * 30)
    print("æ­£åœ¨ç»˜åˆ¶ç†µå‡½æ•°æ›²çº¿...")
    plot_entropy_curve()
    
    # 4. å¯è§†åŒ–ä¿¡æ¯å¢ç›Šæ¯”è¾ƒ
    print("\nğŸ“Š å¯è§†åŒ–ä¿¡æ¯å¢ç›Šæ¯”è¾ƒ")
    print("-" * 30)
    print("æ­£åœ¨ç»˜åˆ¶ä¿¡æ¯å¢ç›Šæ¯”è¾ƒå›¾...")
    plot_information_gain_comparison()
    
    # 5. è¯¦ç»†çš„åˆ†å‰²åˆ†æ
    print("\nğŸ” è¯¦ç»†çš„åˆ†å‰²åˆ†æ")
    print("-" * 30)
    root_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
    
    for feature in range(3):
        feature_names = ["æ£•è‰²èŒç›–", "é”¥å½¢èŒæŸ„", "ç‹¬ç”Ÿ"]
        print(f"\nåˆ†æç‰¹å¾ {feature}: {feature_names[feature]}")
        
        left_indices, right_indices = split_dataset(X_train, root_indices, feature)
        info_gain = compute_information_gain(X_train, y_train, root_indices, feature)
        
        print(f"  ä¿¡æ¯å¢ç›Š: {info_gain:.4f}")
        print(f"  å·¦åˆ†æ”¯æ ·æœ¬æ•°: {len(left_indices)}")
        print(f"  å³åˆ†æ”¯æ ·æœ¬æ•°: {len(right_indices)}")
        
        # è®¡ç®—æ¯ä¸ªåˆ†æ”¯çš„çº¯åº¦
        if len(left_indices) > 0:
            left_edible = sum(y_train[i] for i in left_indices)
            left_purity = left_edible / len(left_indices)
            print(f"  å·¦åˆ†æ”¯å¯é£Ÿç”¨æ¯”ä¾‹: {left_purity:.2%}")
        
        if len(right_indices) > 0:
            right_edible = sum(y_train[i] for i in right_indices)
            right_purity = right_edible / len(right_indices)
            print(f"  å³åˆ†æ”¯å¯é£Ÿç”¨æ¯”ä¾‹: {right_purity:.2%}")
    
    # 6. å†³ç­–æ ‘ç»“æ„åˆ†æ
    print("\nğŸŒ³ å†³ç­–æ ‘ç»“æ„åˆ†æ")
    print("-" * 30)
    tree = []
    
    def build_tree_with_analysis(X, y, node_indices, branch_name, max_depth, current_depth):
        """æ„å»ºæ ‘å¹¶è¿›è¡Œåˆ†æ"""
        if current_depth == max_depth:
            formatting = "  " * current_depth
            edible_count = sum(y[i] for i in node_indices)
            total_count = len(node_indices)
            purity = edible_count / total_count if total_count > 0 else 0
            print(f"{formatting}ğŸ“„ {branch_name} å¶èŠ‚ç‚¹: {total_count} ä¸ªæ ·æœ¬, å¯é£Ÿç”¨æ¯”ä¾‹: {purity:.2%}")
            return
        
        best_feature = get_best_split(X, y, node_indices)
        feature_names = ["æ£•è‰²èŒç›–", "é”¥å½¢èŒæŸ„", "ç‹¬ç”Ÿ"]
        
        formatting = "  " * current_depth
        print(f"{formatting}ğŸ”€ {branch_name} æ·±åº¦ {current_depth}: åˆ†å‰²ç‰¹å¾ {best_feature} ({feature_names[best_feature]})")
        
        left_indices, right_indices = split_dataset(X, node_indices, best_feature)
        tree.append((left_indices, right_indices, best_feature))
        
        build_tree_with_analysis(X, y, left_indices, "å·¦", max_depth, current_depth+1)
        build_tree_with_analysis(X, y, right_indices, "å³", max_depth, current_depth+1)
    
    build_tree_with_analysis(X_train, y_train, root_indices, "æ ¹", max_depth=2, current_depth=0)
    
    # 7. é¢„æµ‹ç¤ºä¾‹
    print("\nğŸ”® é¢„æµ‹ç¤ºä¾‹")
    print("-" * 30)
    
    def predict_sample(sample, tree, root_indices):
        """ä½¿ç”¨å†³ç­–æ ‘é¢„æµ‹å•ä¸ªæ ·æœ¬"""
        current_indices = root_indices.copy()
        
        for left_indices, right_indices, feature in tree:
            if sample[feature] == 1:
                current_indices = [i for i in current_indices if i in left_indices]
            else:
                current_indices = [i for i in current_indices if i in right_indices]
        
        if len(current_indices) == 0:
            return "æ— æ³•ç¡®å®š"
        
        # è®¡ç®—å½“å‰èŠ‚ç‚¹çš„å¤šæ•°ç±»
        edible_count = sum(y_train[i] for i in current_indices)
        total_count = len(current_indices)
        
        if edible_count > total_count / 2:
            return "å¯é£Ÿç”¨"
        elif edible_count < total_count / 2:
            return "æœ‰æ¯’"
        else:
            return "æ— æ³•ç¡®å®š"
    
    # æµ‹è¯•å‡ ä¸ªæ ·æœ¬
    test_samples = [
        [1, 1, 1],  # æ£•è‰²èŒç›–, é”¥å½¢èŒæŸ„, ç‹¬ç”Ÿ
        [0, 0, 0],  # çº¢è‰²èŒç›–, æ‰©å¤§èŒæŸ„, éç‹¬ç”Ÿ
        [1, 0, 1],  # æ£•è‰²èŒç›–, æ‰©å¤§èŒæŸ„, ç‹¬ç”Ÿ
    ]
    
    sample_descriptions = [
        "æ£•è‰²èŒç›– + é”¥å½¢èŒæŸ„ + ç‹¬ç”Ÿ",
        "çº¢è‰²èŒç›– + æ‰©å¤§èŒæŸ„ + éç‹¬ç”Ÿ", 
        "æ£•è‰²èŒç›– + æ‰©å¤§èŒæŸ„ + ç‹¬ç”Ÿ"
    ]
    
    for i, (sample, desc) in enumerate(zip(test_samples, sample_descriptions)):
        prediction = predict_sample(sample, tree, root_indices)
        print(f"æ ·æœ¬ {i+1}: {desc}")
        print(f"  ç‰¹å¾å€¼: {sample}")
        print(f"  é¢„æµ‹ç»“æœ: {prediction}")
        print()
    
    print("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ’¡ æç¤º:")
    print("- è¿è¡Œ 'python test_decision_tree.py' æ¥éªŒè¯å®ç°")
    print("- ä¿®æ”¹ 'decision_tree_lab.py' ä¸­çš„å‚æ•°è¿›è¡Œå®éªŒ")
    print("- æŸ¥çœ‹ 'README.md' äº†è§£é¡¹ç›®è¯¦æƒ…")

if __name__ == "__main__":
    run_complete_demo()
